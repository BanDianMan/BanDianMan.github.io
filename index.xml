<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>我的个人空间</title>
    <link>https://BanDianMan.github.io/</link>
    <description>Recent content on 我的个人空间</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Jun 2020 15:40:51 +0800</lastBuildDate>
    
	<atom:link href="https://BanDianMan.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ApacheOFBiz  初级指南</title>
      <link>https://BanDianMan.github.io/post/apacheofbiz-%E5%88%9D%E7%BA%A7%E6%8C%87%E5%8D%97/</link>
      <pubDate>Thu, 11 Jun 2020 15:40:51 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/apacheofbiz-%E5%88%9D%E7%BA%A7%E6%8C%87%E5%8D%97/</guid>
      <description>Apache OFBiz  介绍 Apache OFBiz是一套业务应用程序，足够灵活，可跨任何行业使用。通用体系结构允许开发人员轻松扩展或增强它以创建自定义功能。OFBiz作为Apache的一个开源项目，从2002年自今已有17年的历史，它每年都会发布一个大版本，中途更新多个小版本它是一个跨平台、跨数据库、跨应用服务器的分布式电子商务类web应用框架，包括实体引擎, 服务引擎, 消息引擎, 工作流引擎, 规则引擎等
特性  强大的JavaWeb框架   OFBiz 是基于 Java 的 Web 框架，包括实体引擎、服务引擎和基于小部件的 UI，允许您快速原型设计和开发 Web 应用程序。
  成熟的 CRM 和 ERP 解决方案   作为 Apache 顶级项目 10 年，OFBiz 已将其作为企业范围 ERP 解决方案的稳定性和成熟度，能够灵活应对您的业务进行更改。
  开发人员友好   OFBiz 架构非常灵活，开发人员能够使用自定义功能快速轻松地扩展和增强框架。
 下载安装 可以采用zip解压的方式进行开发下载地址,也可以从源码仓库clone源码,参考地址
自述文件 欢迎光临ApacheOFBiz一个强大的顶级Apache软件项目。OFBiz是一个用Java编写的企业资源计划(ERP)系统，包含大量的库、实体、服务和特性，以运行业务的各个方面。
 OFBiz文档 OFBiz许可证  系统需求 运行OFBiz的唯一要求是在您的系统上安装了Java Development Kit(JDK) version 8(不仅仅是JRE,而是完整的JDK),您可以从下面的链接下载
 JDK下载   注意 : 如果您正在使用Eclipse,请确保在使用Eclipse创建项目之前运行适当的Eclipse命令gradlew Eclipse,此命令将通过创建类路径和.</description>
    </item>
    
    <item>
      <title>ApacheOozie架构及工作流程模型</title>
      <link>https://BanDianMan.github.io/post/apacheoozie%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Thu, 07 May 2020 10:57:14 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/apacheoozie%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E6%A8%A1%E5%9E%8B/</guid>
      <description>Apache Oozie架构及工作流程模型 一 Oozie概述 1 Oozie是什么  Hadoop的工作流运行引擎和调度程序  MapReduce作业 Spark（Streaming）作业 Hive 作业   Apache顶级项目  打包主要在Hadoop发行版中 http://oozie.apache.org/docs/4.2.0/index.htm   提供工作流管理与协调器 管理Actions的有向无环图（DAG）  2 Oozie架构  运行HTTP服务  Clients通过提交workflows与服务进行交互 工作流程立即执行或以后执行   通过XML定义工作流程  无需编写实现工具接口和扩展Configured类的Java代码   四种访问Oozie的方式  CML Java API Rest API Web UI    3 Oozie组件  工作流定义：这些被表示为有向非循环图（DAG），以指定要执行的动作序列  workflow.xml   Property file：用于workflow的参数  job.properties   库：在lib文件夹中（可选） 协调器：调度程序（可选）  coordinator.xml   捆绑包：协调程序和工作流程的包（可选）  3.</description>
    </item>
    
    <item>
      <title>ApacheFulme基础及案例</title>
      <link>https://BanDianMan.github.io/post/apachefulme%E5%9F%BA%E7%A1%80%E5%8F%8A%E6%A1%88%E4%BE%8B/</link>
      <pubDate>Thu, 07 May 2020 10:49:38 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/apachefulme%E5%9F%BA%E7%A1%80%E5%8F%8A%E6%A1%88%E4%BE%8B/</guid>
      <description>Apache Flume 基础及使用案例 什么是Flume ?   Flume 是用于从多个源将日志流到Hadoop和其他目标的服务。
  一种分布式的、可靠的、可用的服务，用于有效地收集、聚合和移动大量的流数据到Hadoop分布式文件系统(HDFS)。
  Apache Flume具有简单灵活的基于流数据流的体系结构;并且具有可调的故障转移和恢复可靠性机制，具有健壮性和容错性。
  Apahe Flume是做什么的   流数据
 从多个源获取流式数据到Hadoop中存储和分析    隔离系统
 缓冲存储平台的暂态峰值，当传入数据的速率超过数据可以写入目的地的速率    保证数据交付
 使用基于通道的事务来保证可靠的消息传递。    规模水平
 根据需要摄取新的数据流和额外的数据量。    简单的流动  Apache Flume最初是由Cloudera开发的，目的是提供一种快速、可靠地将web服务器生成的大量日志文件流到Hadoop中的方法。  Apahe Flume 架构  Flume作为一种或多种agents部署; Flume agents 是一个JVM进程，它承载组件，事件通过组件从外部源流向下一个目的地。 每个代理包含三个组件  Source(s) , Channel(s) and Sink    多个代理架构 事务  Transaction接口是Flume可靠性的基础 事务在通道中实现。  连接到通道的每个源和接收器都必须获得一个事务对象    Apache Flume 事件   事件是通过系统传递的单个数据包(源&amp;ndash;&amp;gt;信道&amp;ndash;&amp;gt;接收器);</description>
    </item>
    
    <item>
      <title>Cassandra基础及使用</title>
      <link>https://BanDianMan.github.io/post/cassandra%E5%9F%BA%E7%A1%80%E5%8F%8A%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Thu, 07 May 2020 10:46:34 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/cassandra%E5%9F%BA%E7%A1%80%E5%8F%8A%E4%BD%BF%E7%94%A8/</guid>
      <description>Cassandra基础及使用  什么是Apache Cassandra？  Apache Cassandra是一个分布式数据库，用于管理许多商品服务器上的大量数据，同时提供高可用性服务，没有单点故障  分布式分区行存储 物理多数据中心本机支持 连续可用性 线性规模性能 操作简便 易于分发数据 &amp;hellip;    谁在使用Cassandra？  eBay，GitHub，GoDaddy，Netflix，Reddit，Instagram，Intuit，The Weather Channel等超过1500多家公司 苹果的75,000多个节点存储了超过10 PB的数据 Netflix的2,500个节点，420 TB数据，每天超过1万亿个请求 中国搜索引擎Easou（270个节点，300 TB的数据，每天超过8亿个请求） eBay（超过100个节点，250 TB） &amp;hellip;  安装并启动 touch /etc/yum.repos.d/cassandra.repo
vi /etc/yum.repos.d/cassandra.repo
[cassandra] name=Apache Cassandra baseurl=https://www.apache.org/dist/cassandra/redhat/311x/ gpgcheck=1 repo_gpgcheck=1 gpgkey=https://www.apache.org/dist/cassandra/KEYS sudo yum install cassandra
service cassandra start
chkconfig cassandra on # 使Cassandra重新启动后自动启动
Cassandra Keyspaces(键空间)  键空间  是Cassandra中数据的最外层容器/分组，例如合理的数据库/架构 定义节点上的数据复制 列族–是行集合的容器，例如有理表  每行包含有序的列 每个键空间至少具有一个/多个列族   复制因子 复制品放置策略  简单策略 网络拓扑策略     keyspace -&amp;gt; table –&amp;gt; column，对应关系型数据库 database -&amp;gt; table -&amp;gt; column  创建 Cassandra Keyspace   cqlsh</description>
    </item>
    
    <item>
      <title>Scala笔记(一)</title>
      <link>https://BanDianMan.github.io/post/scala%E7%AC%94%E8%AE%B0%E4%B8%80/</link>
      <pubDate>Thu, 07 May 2020 10:32:42 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/scala%E7%AC%94%E8%AE%B0%E4%B8%80/</guid>
      <description>Scala  简介 : Scala是一门多范式的编程语言,一种类似Java的编程语言,设计初衷是实现可伸缩的语言,并集成面向对象编程和函数式编程的各种特性
特征 :  Java和scala可以无缝混编,都是运行在JVM之上的 类型推测(自动推测类型),不用指定类型 并发和分布式(Actor,类似Java多线程Thread) 特质Trait(类似Java中interfaces和abstract结合) 模式匹配,match case(类似Java中的switch case) 高阶函数(函数的参数是函数,函数的返回值和函数),可进行函数式编程  数据类型  Unit表示无值,和其他语言中void等同 Null空值或者空引用 Nothing所有其他类型的子类型,表示没有值 Any 所有类型的超类,任何实例都属于Any类型 AnyRef所有引用类型的超类 AnyVal所有值类型的超类 Boolean布尔类型 String 字符串 char 16bit Unicode字符 double 双精度浮点型 Float 单精度浮点型 Long 有符号数字 Int 整性 Short 16 bit有符号数字 Byte 8bit的有符号数字      Null Trait,其唯一实例为null,是AnyRef的子类,不是AnyVal的子类     Nothing Trait,所有类型(包括AnyRef和AnyVal)的子类,没有实例   None Option的两个子类之一,另一个是Some,用于安全的函数返回值   Unit 无返回值的函数的类型,和Java的void对应   Nil 长度为0的List    函数  普通函数</description>
    </item>
    
    <item>
      <title>Spark笔记(一)</title>
      <link>https://BanDianMan.github.io/post/spark%E7%AC%94%E8%AE%B0%E4%B8%80/</link>
      <pubDate>Thu, 07 May 2020 10:27:18 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/spark%E7%AC%94%E8%AE%B0%E4%B8%80/</guid>
      <description>Scala  简介 : Scala是一门多范式的编程语言,一种类似Java的编程语言,设计初衷是实现可伸缩的语言,并集成面向对象编程和函数式编程的各种特性
特征 :  Java和scala可以无缝混编,都是运行在JVM之上的 类型推测(自动推测类型),不用指定类型 并发和分布式(Actor,类似Java多线程Thread) 特质Trait(类似Java中interfaces和abstract结合) 模式匹配,match case(类似Java中的switch case) 高阶函数(函数的参数是函数,函数的返回值和函数),可进行函数式编程  数据类型  Unit表示无值,和其他语言中void等同 Null空值或者空引用 Nothing所有其他类型的子类型,表示没有值 Any 所有类型的超类,任何实例都属于Any类型 AnyRef所有引用类型的超类 AnyVal所有值类型的超类 Boolean布尔类型 String 字符串 char 16bit Unicode字符 double 双精度浮点型 Float 单精度浮点型 Long 有符号数字 Int 整性 Short 16 bit有符号数字 Byte 8bit的有符号数字      Null Trait,其唯一实例为null,是AnyRef的子类,不是AnyVal的子类     Nothing Trait,所有类型(包括AnyRef和AnyVal)的子类,没有实例   None Option的两个子类之一,另一个是Some,用于安全的函数返回值   Unit 无返回值的函数的类型,和Java的void对应   Nil 长度为0的List    函数  普通函数</description>
    </item>
    
    <item>
      <title>Hadoop生态常用数据模型</title>
      <link>https://BanDianMan.github.io/post/hadoop%E7%94%9F%E6%80%81%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Thu, 07 May 2020 10:26:30 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/hadoop%E7%94%9F%E6%80%81%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/</guid>
      <description>Hadoop常用数据模型 TextFile  文本文件通常采用CSV,JSON等固定长度的纯文本格式 优点  便于与其他应用程序(生成或读取分隔文件)或脚本进行数据交换 易读性好,便于理解   缺点  数据存储量非常庞大 查询效率不高 不支持块压缩     SequenceFile  按行存储键值对为二进制数据格式，以&amp;lt;Key,Value&amp;gt;形式序列化为二进制文件，HDFS自带  支持压缩和分割 Hadoop中的小文件合并 常用于在MapReduce作业之间传输数据   SequenceFile中的Key和Value可以是任意类型的Writable(org.apache.hadoop.io.Writable) Java API : org.apache.hadoop.io.SequenceFile   记录级压缩是如何存储的 ？
 A：记录级仅压缩value数据，按byte的偏移量索引数据。每个记录头为两个固定长度为4的数据量，一个代表本条Record的长度，一个代表Key值的长度，随后依次存储key值和压缩的value值。
模拟读取过程如下，首先偏移4获得本条记录长度r，然后偏移4获得key类型的长度k，再偏移k读入key的值，最后偏移到位置r，获取压缩后的value值，本条记录读完。
 块级压缩是如何存储的 ?
 A：块级压缩同时压缩key和value，相同特性的记录将会被归为同一块。块头为定长4byte的代表本块所含的记录数目n、定长4byte的代表压缩后的key的长度的k、压缩后的key值组、定长4byte的达标压缩后的value的长度的v、压缩后的value值组。（n,k_length,k, v_length,v）
模拟读取过程如下，首先偏移4byte，获取当前块存储的记录数目n；偏移4byte，获取压缩后的key类型的长度k，再偏移nk读入多个key值分别存储；偏移4byte，获取压缩后的value类型的长度v，再偏移nv读入多个value值分别对应key值。
读写操作  读写操作  SequenceFile.Writer （指定为块压缩） SequenceFile.Reader（读取时能够自动解压）   在Hive中使用SequenceFile 方式一  STORED AS sequencefile   方式二显示指定  STORED AS INPUTFORMAT &amp;#39;org.apache.hadoop.mapred.SequenceFileInputFormat&amp;#39; OUTPUTFORMAT &amp;#39;org.</description>
    </item>
    
    <item>
      <title>Hive的环境部署</title>
      <link>https://BanDianMan.github.io/post/hive%E7%9A%84%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Thu, 07 May 2020 10:03:48 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/hive%E7%9A%84%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/</guid>
      <description>Hive安装文档 hive的安装部署  由于hive是依赖于hadoop的, 所以先把hadoop相关的服务启动 配置hive &amp;ndash;&amp;gt;解压 tar -zxvf apache-hive-1.2.1-bin.tar.gz -C [一个安装目录] &amp;ndash;&amp;gt;创建目录用于保存hive的所有数据, 便于管理   bin/hdfs dfs -mkdir /tmp bin/hdfs dfs -mkdir /user/hive/warehouse &amp;ndash;&amp;gt;修改权限
 bin/hdfs dfs -chmod g+w /tmp bin/hdfs dfs -chmod g+w /user/hive/warehouse  hive在HDFS上的默认路径
 &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hive.metastore.warehouse.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;/user/hive/warehouse&amp;lt;/value&amp;gt; &amp;lt;description&amp;gt;location of default database for the warehouse&amp;lt;/description&amp;gt; &amp;lt;/property&amp;gt;   修改hive-env.sh(改名)
   # Set HADOOP_HOME to point to a specific hadoop install directory HADOOP_HOME=/opt/moduels/hadoop-2.5.0 # Hive Configuration Directory can be controlled by: export HIVE_CONF_DIR=/opt/moduels/hive-0.</description>
    </item>
    
    <item>
      <title>Hive的调优</title>
      <link>https://BanDianMan.github.io/post/hive%E7%9A%84%E8%B0%83%E4%BC%98/</link>
      <pubDate>Thu, 07 May 2020 10:02:47 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/hive%E7%9A%84%E8%B0%83%E4%BC%98/</guid>
      <description>表的优化   在表的优化中, 当数据量较大的时候常用的手段就是拆分表, 大表拆小表, 分区表, 临时表, 外部表
  小表和大表的join, 要把数据量小的表放在join的左边, 先进行缓存, 这样会减少表join的时候内存的消耗量
  数据倾斜 数据倾斜产生的原因为分区之后某一个reduce运算的数据量比较小, 而某一个reduce运行的数据量比较大, 造成两个reduce处理数据不平等
合理设置map数量 可以影响map的数量的因素 在input文件夹中, 每一个文件就是一个map. 而input文件的数量, input文件的大小都会影响map的数量, 在mapreduce任务中, 一个切片就是一个map任务, 在Driver中设置如下:
FileInputFormat.setMaxInputSplitSize(job, size); FileInputFormat.setMinInputSplitSize(job, size); 合理设置reduce数量 设置reduce个数:
hive (default)&amp;gt; set mapreduce.job.reduces; mapreduce.job.reduces=-1 //默认为-1, 就是不设置reduce的个数 根据业务自定分区规则
并行执行 并行执行与java多线程的异步和同步概念差不多, 在MR运行任务中, 存在很多的MR任务可以进行执行, 有些MR任务和下一个MR任务存在依赖关系, 但是有些MR任务没有依赖关系. 例如: 存在依赖关系的MR, 一个MR任务的输出就是下一个MR任务的输入. 对于没有依赖关系的MR任务可以使用并行执行, 在同一时间运行多个MR任务, 这样在运行的过程中效率就会得到提升.
可以通过以下参数来设置
 开启并行任务  hive (default)&amp;gt; set hive.exec.parallel; hive.exec.parallel=false --------------------------------------- set hive.exec.parallel=true; 设置多少个任务可以同时运行  hive (default)&amp;gt; set hive.</description>
    </item>
    
    <item>
      <title>ApacheKafka基础及开发</title>
      <link>https://BanDianMan.github.io/post/apachekafka%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%BC%80%E5%8F%91/</link>
      <pubDate>Thu, 07 May 2020 09:53:22 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/apachekafka%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%BC%80%E5%8F%91/</guid>
      <description>Apache Kafka 基础及开发 介绍   Apache Kafka是一个高吞吐量的分布式发布-订阅消息系统，它被设计成可快速扩展和持久的
  Kafka经常被用来代替传统的消息代理，比如JMS和AMQP，因为它具有更高的吞吐量、可靠性和可复制性。
  Kafika是用Scala写的。
  特性  快 : 一个Kafka代理可以处理数千个客户端每秒数百兆的读写可 扩展 : Kafka集群可以弹性地、透明地扩展，而不需要停机。 经久耐用 : 消息被保存在磁盘上，并在集群内复制 实时 : Kafka用于构建实时数据管道和流媒体应用 容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）  架构  Kafka在名为主题的类别中维护消息提要; 制作人将消息发布到一个Kafka主题; 使用者订阅主题并处理已发布消息的提要; Kafka集群中的服务器称为代理。  使用场景   日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等。
  消息系统：解耦和生产者和消费者、缓存消息等。
  用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。
  运营指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。
  流式处理：比如spark streaming和storm
  事件源
  高吞吐量和低延迟 Kafka主要通过两个关键概念来实现高吞吐量和低延迟
  分批处理单个消息以分摊网络开销和附加/消耗块
  使用sendfile的零拷贝l/O, (Java的NIO FileChannel transferTo方法)</description>
    </item>
    
    <item>
      <title>Hive笔记(一)</title>
      <link>https://BanDianMan.github.io/post/hive%E7%AC%94%E8%AE%B0%E4%B8%80/</link>
      <pubDate>Thu, 07 May 2020 09:34:52 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/hive%E7%AC%94%E8%AE%B0%E4%B8%80/</guid>
      <description>Hive概述 什么是Hive ?  Hives是基于Hadoop构建的一个数据仓库工具 可以将结构化的数据映射为一张数据库表 提供HQL(HiveSQL)查询功能 由Facebook实现并开源 底层数据存储在HDFS上 Hive的本质是将SQL语句转换为MapReduce任务运行 使不熟悉MapReduce的用户很方便的利用HQL处理和计算HDFS上的结构化数据,适用于离线的批量数据计算   Hive 依赖于HDFS存储数据,Hive将HQL转换成MapReduce执行,所以说Hive是基于Hadoop的一个数据仓库工具,实质就是一款基于HDFS的MapReduce的计算框架,对存储在HDFS中的数据进行分析和管理  为什么使用Hive ?  友好的接口 : 操作接口采用类似SQL的语法,提供快速开发的能力 低学习成本 : 避免了写MapReduce,减少开发人员的学习成本 好的扩展性 : 可自由的扩展集群规模而无需重启服务,支持用户自定义函数  Hive的特点 优点 :  可扩展性 简化MR开发 自定义函数,格式 庞大活跃的社区  缺点 :  不支持记录级别的增删改查操作 查询延时严重 不支持事物  Hive与RDBMS 的对比 数据表(Tables)  分为内部表和外部表 内部表(管理表)  HDFS中为所属数据库目录下的子文件夹 数据完全由Hive管理,删除表(元数据) 会删除数据   外部表 (External Tables)  数据保存在指定位置的HDFS路径中 Hive不完全管理数据,删除表(元数据)不会删除数据    内部表和外部表的区别  删除内部表，删除表元数据和数据 删除外部表，删除元数据，不删除数据  内部表和外部表的使用选择 如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表，但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。</description>
    </item>
    
    <item>
      <title>深度学习之图像处理与分析</title>
      <link>https://BanDianMan.github.io/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%8E%E5%88%86%E6%9E%90/</link>
      <pubDate>Thu, 07 May 2020 09:09:34 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%8E%E5%88%86%E6%9E%90/</guid>
      <description>深度学习之图像处理与分析  目录  人工智能和深度学习概论 图像基础 深度学习基础 深度学习的基本数学 理解的人工神经网络  人工智能和深度学习概论 中国的AI  2017年7月，国务院发布白皮书，使中国到2030年成为全球AI领导者，行业价值1500亿美元 到2030年投资70亿美元，其中包括在北京的一个研究园的20亿美元 全球AI资金占主导地位48％，而美国在2017年为38％ 中国的AI公司总数为23％，而美国2017年为42％  AI与ML与DL  Artificial intelligence(人工智能)  使人类通常执行的智力任务自动化的努力   Machine Learning(机器学习)  使系统无需进行显式编程即可自动从数据进行改进   Deep Learning(深度学习)  机器学习的特定子领域 侧重于学习越来越有意义的表示形式的连续层    人工神经网络  最初于1950年代进行调查，始于1980年代 不是真正的大脑模型 受到神经生物学研究的宽松启发  深度学习的深度如何？  深度学习是人工神经网络的重塑，具有两层以上 “深入”并不是指通过这种方法获得的更深刻的理解 它代表连续表示层的想法  深度学习框架 我们的深度学习技术堆栈 GPU和CUDA  GPU(Graphics Processing Unit):  数百个更简单的内核 数千个并发的硬件线程 最大化浮点吞吐量   CUDA(Compute Unified Device Architecture)  并行编程模型，可通过利用GPU显着提高计算性能   cuDNN（CUDA深度神经网络库）  GPU加速的神经网络原语库 它为以下方面提供了高度优化的实现：卷积，池化，规范化和激活层    设置深度学习环境  安装Anaconda3-5.</description>
    </item>
    
    <item>
      <title>基于SparkStreaming的流数据处理和分析</title>
      <link>https://BanDianMan.github.io/post/%E5%9F%BA%E4%BA%8Esparkstreaming%E7%9A%84%E6%B5%81%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%92%8C%E5%88%86%E6%9E%90/</link>
      <pubDate>Wed, 06 May 2020 20:11:44 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/%E5%9F%BA%E4%BA%8Esparkstreaming%E7%9A%84%E6%B5%81%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%92%8C%E5%88%86%E6%9E%90/</guid>
      <description>基于Spark Streaming的流数据处理和分析 一 Spark Streaming 1 Spark Streaming概述 1.1 实时数据处理的动机  以前所未有的速度创造数据  来自移动，网络，社交，物联网的指数数据增长&amp;hellip; 联网设备：2012年为9B，到2020年将达到50B 到2020年，超过1万亿个传感器   我们如何实时利用数据的价值？  价值会迅速下降→立即获取价值 从被动分析到直接运营 解锁新的竞争优势 需要全新的方法    1.2 跨行业的用例 1.3 什么是Spark Streaming？ Apache Spark核心API的扩展，用于流处理。该框架提供具有良好的容错能力、可扩展性、高通量、低延迟的优点
1.4 流引擎对比 1.5 流处理架构 1.6 微批量架构  传入数据作为离散流（DStream） 流被细分为微批。从Spark 2.3.1起延迟可达到1毫秒（在此之前大约100毫秒） 每个微批处理都是一个RDD –可以在批处理和流之间共享代码  2 Spark Streaming 操作 2.1 Streaming Context Streaming Context消费Spark中的数据流，数据流输入后， Streaming Context会将数据流分成批数据
 一个JVM中只能激活一个StreamingContext StreamingContext在停止后无法重新启动，但可以重新创建  2.2 DStream Discretized Stream（离散流）或DStream是Spark Streaming提供的基本抽象
2.2.1 Input DStreams 和 Receivers Streaming Context只能在Driver端，Receiver可以在Executor端</description>
    </item>
    
    <item>
      <title>ApacheNiFi介绍</title>
      <link>https://BanDianMan.github.io/post/apachenifi%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Wed, 06 May 2020 15:23:58 +0800</pubDate>
      
      <guid>https://BanDianMan.github.io/post/apachenifi%E4%BB%8B%E7%BB%8D/</guid>
      <description>Apache NiFi基础及架构  Apache NiFi介绍  Apache NiFi是一个易于使用，功能强大且可靠的系统，可通过编排和执行数据流来处理和分发数据  Apache NiFiis使系统之间的数据流自动化 基于Web的用户界面 高度可配置  容忍损失与保证交付 低延迟与高吞吐量   资料来源  从头到尾跟踪数据流   专为扩展而设计  构建定制处理器   安全  SSL，SSH，HTTPS，加密内容等      NiFi子项目-MiNiFi  MiNiFi是一种补充性的数据收集方法，是对NiFiin数据流管理的核心原则的补充，侧重于在创建源时收集数据  NiFilives位于数据中心中-为它提供企业服务器或它们的集群 MiNiFiLive距离数据出生地很近，并且是该设备或系统上的访客    NiFi功能  保证交付 数据缓冲 优先排队 流特定的QoS  延迟与吞吐量 损耗容限   资料来源 支持推拉模型 视觉命令与控制 流模板 可插拔/多角色安全 专为扩展而设计  丰富的内置处理器 广泛的支持和3rdcustom处理器   聚类  核心概念  FlowFile – 表示在系统中移动的每个对象 Proessor(处理器) – 执行实际工作，例如系统之间的数据路由，转换或中介  处理器可以访问给定FlowFile及其内容流的属性 处理器在给定单元中进行提交或回滚工作   Connection(连接) – 通过充当队列来提供处理器之间的实际链接 Flow Controller(流控制器)–充当代理，促进按计划在处理器之间交换FlowFiles Process Group(流程组) – 是一组特定的流程及其联系  允许创建可重复使用的组件    Apache NiFi架构 Apache NiFi集群  零主集群 每个节点对数据执行相同的任务，但对不同的数据集执行相同的任务 管理员选择了一个集群协调员，负责协调集群中各节点之间的连接性。  NiFi Site-to-Site  两个NiFi instances之间的直接通信 推到接收器上的输入端口，或从信源上的输出端口拉 处理负载平衡和可靠的交付 使用证书的安全连接（可选）  Site-to-Site Push  源将远程进程组连接到目标上的输入端口 Site-to-Site 负责群集中各个节点之间的负载平衡  Site-to-Site Pull  目标将远程进程组连接到源上的输出端口 如果源是集群，则每个节点将从群集中的每个节点中提取  Site-to-Site Client  任何要推动或退出NiFi的客户  Flow File  FlowFile表示在系统中移动的每个对象  Header: key/value 对属性 Content: 零个或多个字节    FlowFileProcessor  处理器实际执行工作:  操作FlowFilecontent  数据路由，数据转换等   提交或回滚工作 使用/添加/更新属性    Connection  连接提供了处理器之间的实际联系  这些充当队列，并允许各种进程以不同的速率进行交互 可以动态地对这些队列进行优先级排序，并可以在负载上设置上限，从而实现Back Pressure  Back Pressure是指在不再计划运行作为连接源的组件之前，应允许队列中存在多少数据 NiFi提供了两个Back Pressure配置元素  FolwFiles数 数据大小        Flow Controller  NiFi dataflow调度程序和执行引擎:  维护有关进程如何连接和管理所有进程使用的线程及其分配的知识 充当代理，促进处理器之间的FlowFile交换    Process Group  流程组是一组特定的流程及其连接  通过输入端口接收数据，并通过输出端口发送数据 流程组允许仅通过组合其他组件来创建全新的组件  输入口 输出口      Processors(处理器)  数据提取处理器  ListSFTP, FetchSFTP, ListFiles, FetchFiles, GetFile, ListHDFS, FetchHDFS, &amp;hellip; etc   路由和中介处理器  路由和中介处理器   数据库访问处理器  ExecuteSQL，PutSQL，ListDatabaseTables等   属性提取处理器  UpdateAttribute，EvaluateJSONPath，ExtractText等   系统交互处理器  ExecuteScript，ExecuteProcess，ExecuteStreamCommand等   数据转换处理器  ReplaceText，SplitText，MergeContent，&amp;hellip;   发送数据处理器  PutSFTP, PutFile, PutHDFS, PublishKafka, ConsumeKafka, &amp;hellip;    处理器关系  流文件通过使用处理器之间的关系进行验证的连接从一个处理器移动到另一个处理器  每当建立连接时，开发人员都会在这些处理器之间选择一个或多个关系 SUCCESS  流文件成功完成流程后移至下一个流程   FAILURE  流文件在失败的流程后移至下一个流程      DEMO &amp;hellip;</description>
    </item>
    
  </channel>
</rss>